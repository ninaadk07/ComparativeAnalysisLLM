{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NaE6jgBxxyef"
      },
      "outputs": [],
      "source": [
        "!pip install fredapi requests yahoo_fin bs4 tabulate openai==0.28 anthropic google-generativeai rouge_score\n",
        "\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import random\n",
        "from fredapi import Fred\n",
        "import requests\n",
        "from yahoo_fin import news\n",
        "from bs4 import BeautifulSoup\n",
        "from tabulate import tabulate\n",
        "import random\n",
        "import openai\n",
        "import anthropic\n",
        "#from llama_cpp import Llama\n",
        "import google.generativeai as genai\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "baBhwg_xcuRW"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from yahoo_fin import news\n",
        "from datetime import datetime, timedelta\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import time\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "qW34_Iqe4MiE"
      },
      "outputs": [],
      "source": [
        "#Financial News Articles - For Stock (readily available w/ ticker)\n",
        "from yahoo_fin import news\n",
        "\n",
        "latest_news = news.get_yf_rss('AAPL')\n",
        "\n",
        "for i, article in enumerate(latest_news[:10], start=1):\n",
        "    print(f\"{i}. {article['title']}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GzjNzoxo4bGU"
      },
      "outputs": [],
      "source": [
        "#preprocessing training data\n",
        "nltk_resources = [\"punkt\", \"stopwords\", \"wordnet\"]\n",
        "\n",
        "for resource in nltk_resources:\n",
        "    try:\n",
        "        nltk.data.find(f\"tokenizers/{resource}\")\n",
        "    except LookupError:\n",
        "        print(f\"üîπ Downloading missing NLTK resource: {resource}\")\n",
        "        nltk.download(resource)\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "import re\n",
        "import random\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from yahoo_fin import news\n",
        "from datetime import datetime, timedelta\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# üîπ Download necessary NLTK resources (only needed once)\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# üîπ Initialize NLP components\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# üîπ List of S&P 500 stock tickers (expandable)\n",
        "sp500_tickers = [\n",
        "    \"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"NVDA\", \"FB\", \"JPM\", \"V\", \"DIS\",\n",
        "    \"PG\", \"MA\", \"NFLX\", \"INTC\", \"PYPL\", \"PFE\", \"PEP\", \"KO\", \"CSCO\", \"XOM\",\n",
        "    \"IBM\", \"GE\", \"BA\", \"GS\", \"C\", \"ADBE\", \"NFLX\", \"T\", \"UNH\", \"MRK\",\n",
        "    \"MMM\", \"LMT\", \"MDT\", \"CAT\", \"MO\", \"HON\", \"F\", \"GM\", \"MCD\", \"WMT\",\n",
        "    \"NKE\", \"CVX\", \"TMO\", \"ORCL\", \"DOW\", \"SO\", \"DUK\", \"AEP\", \"USB\", \"MET\",\n",
        "    \"BK\", \"TGT\", \"BLK\", \"SCHW\", \"CME\", \"AON\", \"SPGI\", \"MS\", \"AXP\", \"CI\",\n",
        "    \"ISRG\", \"ZTS\", \"PLD\", \"DE\", \"ADP\", \"SYK\", \"GILD\", \"GM\", \"TFC\", \"CSX\",\n",
        "    \"EL\", \"FDX\", \"EMR\", \"ECL\", \"ADI\", \"MMC\", \"WM\", \"AMT\", \"SLB\", \"AIG\",\n",
        "    \"CTSH\", \"DHR\", \"NSC\", \"COF\", \"DG\", \"FIS\", \"PGR\", \"ITW\", \"CDNS\", \"APD\",\n",
        "    \"AFL\", \"OXY\", \"VLO\", \"HES\", \"PSX\", \"TRGP\", \"MRO\", \"HAL\", \"NEM\", \"BKR\"\n",
        "]\n",
        "\n",
        "# üîπ Function to generate a random date between 2011 and 2021\n",
        "def random_date():\n",
        "    start_date = datetime(2011, 1, 1)\n",
        "    end_date = datetime(2021, 12, 31)\n",
        "    return start_date + timedelta(days=random.randint(0, (end_date - start_date).days))\n",
        "\n",
        "# üîπ Function to clean and preprocess text\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    words = word_tokenize(text)  # Tokenization\n",
        "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatization\n",
        "    return \" \".join(words)\n",
        "\n",
        "# üîπ Fetch and process financial news for at least 100 random ticker-date pairs\n",
        "news_data = []\n",
        "attempts = 0\n",
        "\n",
        "while len(news_data) < 5000 and attempts < 20000:\n",
        "    attempts += 1\n",
        "    ticker = random.choice(sp500_tickers)\n",
        "    date = random_date().strftime('%Y-%m-%d')\n",
        "\n",
        "    try:\n",
        "\n",
        "        latest_news = news.get_yf_rss(ticker)\n",
        "\n",
        "        if latest_news:\n",
        "            for article in latest_news[:1]:\n",
        "                title = article[\"title\"]\n",
        "                link = article[\"link\"]\n",
        "                cleaned_title = clean_text(title)\n",
        "\n",
        "                news_data.append({\n",
        "                    \"Ticker\": ticker,\n",
        "                    \"Date\": date,\n",
        "                    \"Original Title\": title,\n",
        "                    \"Cleaned Title\": cleaned_title,\n",
        "                    \"Link\": link\n",
        "                })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error fetching news for {ticker} on {date}: {e}\")\n",
        "\n",
        "df_news = pd.DataFrame(news_data)\n",
        "\n",
        "import tabulate\n",
        "print(\"\\nüìä Processed Historical Financial News:\\n\")\n",
        "print(tabulate.tabulate(df_news.head(100), headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
        "\n",
        "df_news.to_csv(\"historical_financial_news.csv\", index=False)\n",
        "print(\"Processed financial news saved to 'historical_financial_news.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zfZKUMyhykxP"
      },
      "outputs": [],
      "source": [
        "#preprocessing evaluation data\n",
        "nltk_resources = [\"punkt\", \"stopwords\", \"wordnet\"]\n",
        "\n",
        "for resource in nltk_resources:\n",
        "    try:\n",
        "        nltk.data.find(f\"tokenizers/{resource}\")\n",
        "    except LookupError:\n",
        "        print(f\"üîπ Downloading missing NLTK resource: {resource}\")\n",
        "        nltk.download(resource)\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "\n",
        "# üîπ Download necessary NLTK resources (only needed once)\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "# üîπ Initialize NLP components\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# üîπ List of S&P 500 stock tickers (expandable)\n",
        "sp500_tickers = [\n",
        "    \"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"TSLA\", \"NVDA\", \"FB\", \"JPM\", \"V\", \"DIS\",\n",
        "    \"PG\", \"MA\", \"NFLX\", \"INTC\", \"PYPL\", \"PFE\", \"PEP\", \"KO\", \"CSCO\", \"XOM\",\n",
        "    \"IBM\", \"GE\", \"BA\", \"GS\", \"C\", \"ADBE\", \"NFLX\", \"T\", \"UNH\", \"MRK\",\n",
        "    \"MMM\", \"LMT\", \"MDT\", \"CAT\", \"MO\", \"HON\", \"F\", \"GM\", \"MCD\", \"WMT\",\n",
        "    \"NKE\", \"CVX\", \"TMO\", \"ORCL\", \"DOW\", \"SO\", \"DUK\", \"AEP\", \"USB\", \"MET\",\n",
        "    \"BK\", \"TGT\", \"BLK\", \"SCHW\", \"CME\", \"AON\", \"SPGI\", \"MS\", \"AXP\", \"CI\",\n",
        "    \"ISRG\", \"ZTS\", \"PLD\", \"DE\", \"ADP\", \"SYK\", \"GILD\", \"GM\", \"TFC\", \"CSX\",\n",
        "    \"EL\", \"FDX\", \"EMR\", \"ECL\", \"ADI\", \"MMC\", \"WM\", \"AMT\", \"SLB\", \"AIG\",\n",
        "    \"CTSH\", \"DHR\", \"NSC\", \"COF\", \"DG\", \"FIS\", \"PGR\", \"ITW\", \"CDNS\", \"APD\",\n",
        "    \"AFL\", \"OXY\", \"VLO\", \"HES\", \"PSX\", \"TRGP\", \"MRO\", \"HAL\", \"NEM\", \"BKR\"\n",
        "]\n",
        "\n",
        "# üîπ Function to generate a random date between 2011 and 2021\n",
        "def random_date():\n",
        "    start_date = datetime(2022, 1, 1)\n",
        "    end_date = datetime(2024, 12, 31)\n",
        "    return start_date + timedelta(days=random.randint(0, (end_date - start_date).days))\n",
        "\n",
        "# üîπ Function to clean and preprocess text\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    words = word_tokenize(text)  # Tokenization\n",
        "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]  # Lemmatization\n",
        "    return \" \".join(words)\n",
        "\n",
        "# üîπ Fetch and process financial news for at least 100 random ticker-date pairs\n",
        "news_data = []\n",
        "attempts = 0\n",
        "\n",
        "while len(news_data) < 1500 and attempts < 20000:\n",
        "    attempts += 1\n",
        "    ticker = random.choice(sp500_tickers)\n",
        "    date = random_date().strftime('%Y-%m-%d')\n",
        "\n",
        "    try:\n",
        "\n",
        "        latest_news = news.get_yf_rss(ticker)\n",
        "\n",
        "        if latest_news:\n",
        "            for article in latest_news[:1]:\n",
        "                title = article[\"title\"]\n",
        "                link = article[\"link\"]\n",
        "                cleaned_title = clean_text(title)\n",
        "\n",
        "                news_data.append({\n",
        "                    \"Ticker\": ticker,\n",
        "                    \"Date\": date,\n",
        "                    \"Original Title\": title,\n",
        "                    \"Cleaned Title\": cleaned_title,\n",
        "                    \"Link\": link\n",
        "                })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error fetching news for {ticker} on {date}: {e}\")\n",
        "\n",
        "df_news = pd.DataFrame(news_data)\n",
        "\n",
        "import tabulate\n",
        "print(\"\\nüìä Processed Recent Financial News:\\n\")\n",
        "print(tabulate.tabulate(df_news.head(100), headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
        "\n",
        "df_news.to_csv(\"evaluation_financial_news.csv\", index=False)\n",
        "print(\"Processed financial news saved to 'evaluation_financial_news.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FBPl7720jZf8"
      },
      "outputs": [],
      "source": [
        "#vader sentiment analysis on training data\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "df_news = pd.read_csv(\"historical_financial_news.csv\")\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "df_news[\"Sentiment Score\"] = df_news[\"Cleaned Title\"].apply(lambda text: sia.polarity_scores(str(text))[\"compound\"])\n",
        "\n",
        "df_news[\"Sentiment\"] = df_news[\"Sentiment Score\"].apply(\n",
        "    lambda score: \"Positive\" if score > 0.05 else \"Negative\" if score < -0.05 else \"Neutral\"\n",
        ")\n",
        "\n",
        "print(\"\\nüìä VADER Sentiment Analysis Results:\\n\")\n",
        "print(df_news[[\"Ticker\", \"Date\", \"Cleaned Title\", \"Sentiment Score\", \"Sentiment\"]].head(10))\n",
        "\n",
        "df_news.to_csv(\"vader_sentiment_analysis_results.csv\", index=False)\n",
        "print(\"\\n‚úÖ Sentiment analysis results saved to 'vader_sentiment_analysis_results.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sHgziNhex6yw"
      },
      "outputs": [],
      "source": [
        "#VADER sentiment analysis on evaluation data\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "df_eval_news = pd.read_csv(\"evaluation_financial_news.csv\")\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "df_eval_news[\"Sentiment Score\"] = df_eval_news[\"Cleaned Title\"].apply(lambda text: sia.polarity_scores(str(text))[\"compound\"])\n",
        "\n",
        "df_eval_news[\"Sentiment\"] = df_eval_news[\"Sentiment Score\"].apply(\n",
        "    lambda score: \"Positive\" if score > 0.05 else \"Negative\" if score < -0.05 else \"Neutral\"\n",
        ")\n",
        "\n",
        "print(\"\\nüìä VADER Sentiment Analysis Results (2022-2024):\\n\")\n",
        "print(df_eval_news[[\"Ticker\", \"Date\", \"Cleaned Title\", \"Sentiment Score\", \"Sentiment\"]].head(10))\n",
        "\n",
        "df_eval_news.to_csv(\"vader_sentiment_analysis_results_2022_2024.csv\", index=False)\n",
        "print(\"\\n‚úÖ Sentiment analysis results saved to 'vader_sentiment_analysis_results_2022_2024.csv'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aBQoye5Djj59"
      },
      "outputs": [],
      "source": [
        "#Training of Anthropic Claude 3.7 using Chain of Thought prompting and Reinforcement Learning\n",
        "import time\n",
        "begin = time.time()\n",
        "df_news = pd.read_csv(\"historical_financial_news.csv\")\n",
        "df_vader = pd.read_csv(\"vader_sentiment_analysis_results.csv\")\n",
        "\n",
        "# Initialize Anthropic Claude API\n",
        "CLAUDE_API_KEY = \"\"\n",
        "  # Replace with actual API key\n",
        "client = anthropic.Anthropic(api_key=CLAUDE_API_KEY)\n",
        "\n",
        "# Initialize reward tracking\n",
        "reward_score = 0  # Start with neutral reward score\n",
        "previous_vader = None\n",
        "previous_claude = None\n",
        "\n",
        "# Chain-of-Thought (CoT) Prompting Template\n",
        "cot_prompt_template = \"\"\"\n",
        "You are an AI trained in financial sentiment analysis.\n",
        "Analyze the sentiment of the following financial news article step by step. Limit your answer in total to 500 characters.\n",
        "\n",
        "1. **Extract Key Phrases**: Identify important financial terms, events, and entities.\n",
        "2. **Determine Sentiment Score**: Provide a sentiment score between -1 (Negative), 0 (Neutral), and +1 (Positive). Do it to 2 decimal places\n",
        "3. **Explain Sentiment**: Justify the assigned sentiment score based on the financial impact.\n",
        "\n",
        "Example Sentiment Analysis:\n",
        "\n",
        "Article: \"{article}\"\n",
        "---\n",
        "Step 1: Key Phrases ‚Üí [extracted key phrases]\n",
        "Step 2: Sentiment Score ‚Üí [Numeric value between -1 and 1]\n",
        "Step 3: Explanation ‚Üí [detailed reasoning]\n",
        "\n",
        "Final Answer: [Numeric Sentiment Score]\n",
        "\"\"\"\n",
        "\n",
        "# Reinforcement Learning Loop\n",
        "for index, row in df_news.iterrows():\n",
        "    article_text = row[\"Cleaned Title\"]\n",
        "\n",
        "    # Retrieve VADER sentiment score\n",
        "    vader_sentiment = df_vader.loc[df_vader[\"Ticker\"] == row[\"Ticker\"], \"Sentiment Score\"].values[0]\n",
        "\n",
        "    # Update CoT Prompt with Reinforcement Learning Adjustment and Previous Scores\n",
        "    updated_prompt = cot_prompt_template\n",
        "\n",
        "    if reward_score < 0.2 and previous_vader is not None and previous_claude is not None:\n",
        "        updated_prompt += f\"\\n\\n‚ö†Ô∏è Previous predictions have been inaccurate. Pay closer attention to sentiment context.\"\n",
        "        updated_prompt += f\" The last article had a VADER sentiment score of {previous_vader} and you provided {previous_claude}.\"\n",
        "    elif reward_score > 0.4 and previous_vader is not None and previous_claude is not None:\n",
        "        updated_prompt += f\"\\n\\n‚úÖ Previous predictions have been accurate. Keep applying the same logic.\"\n",
        "        updated_prompt += f\" The last article had a VADER sentiment score of {previous_vader} and you provided {previous_claude}.\"\n",
        "\n",
        "    # Format the prompt with the financial news article\n",
        "    formatted_prompt = updated_prompt.format(article=article_text)\n",
        "\n",
        "    # Send the CoT prompt to Claude AI\n",
        "    response = client.messages.create(\n",
        "        model=\"claude-3-7-sonnet-20250219\",\n",
        "        max_tokens=500,\n",
        "        temperature=0.5,\n",
        "        messages=[{\"role\": \"user\", \"content\": formatted_prompt}]\n",
        "    )\n",
        "\n",
        "    # Extract Claude's sentiment prediction using string splitting\n",
        "    try:\n",
        "        claude_sentiment = float(response.content[0].text.split(\"Final Answer: \")[-1].strip())\n",
        "    except ValueError:\n",
        "        claude_sentiment = None  # If extraction fails, set to None\n",
        "\n",
        "    # Reward/Penalty Mechanism (Calculate Absolute Difference)\n",
        "    if claude_sentiment is not None:\n",
        "        score_difference = abs(claude_sentiment - vader_sentiment)  # Closer = better\n",
        "        reward = 1 - score_difference  # Higher reward for smaller difference\n",
        "        reward_score += reward\n",
        "\n",
        "        print(f\"üì∞ Article: {article_text}\")\n",
        "        print(f\"‚úÖ Claude Score: {claude_sentiment} | VADER Score: {vader_sentiment} | Reward: {reward:.2f}\")\n",
        "\n",
        "        # Store previous scores for next iteration\n",
        "        previous_vader = vader_sentiment\n",
        "        previous_claude = claude_sentiment\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Warning! Could not extract sentiment from Claude's response.\")\n",
        "\n",
        "    # Adjust reward tracking\n",
        "    print(f\"üîπ Current Reward Score: {reward_score:.2f}\\n\")\n",
        "\n",
        "print(\"\\n‚úÖ Reinforcement Learning complete! Claude AI has processed all 100 financial news articles with adaptive sentiment refinement.\")\n",
        "\n",
        "df_evaluation = pd.read_csv(\"evaluation_financial_news.csv\")\n",
        "\n",
        "evaluation_prompt_template = \"\"\"\n",
        "You have earlier been provided with a training dataset consisting of equity based financial articles from 2011-2021.\n",
        "You were asked to perform sentiment analysis on those articles, while being guided as to how accurate your predictions were\n",
        "Now, you will be prompted with a set of financial articles from 2022-2024, and use the information you gained from the training process to do the following:\n",
        "Analyze the sentiment of the following financial news article step by step. Limit your answer in total to 500 characters.\n",
        "\n",
        "1. **Extract Key Phrases**: Identify important financial terms, events, and entities.\n",
        "2. **Determine Sentiment Score**: Provide a sentiment score between -1 (Negative), 0 (Neutral), and +1 (Positive). Please do this to 4 decimal places\n",
        "3. **Explain Sentiment**: Justify the assigned sentiment score based on the financial impact.\n",
        "\n",
        "Example Sentiment Analysis:\n",
        "\n",
        "Article: \"{article}\"\n",
        "---\n",
        "Step 1: Key Phrases ‚Üí [extracted key phrases]\n",
        "Step 2: Sentiment Score ‚Üí [Numeric value between -1 and 1]\n",
        "Step 3: Explanation ‚Üí [detailed reasoning]\n",
        "\n",
        "Final Answer: [Numeric Sentiment Score]\n",
        "\"\"\"\n",
        "\n",
        "sentiment_results = []\n",
        "explanation_results = []\n",
        "\n",
        "for index, row in df_eval_news.iterrows():\n",
        "    article_text = row[\"Cleaned Title\"]\n",
        "\n",
        "    # Format the prompt with the financial news article\n",
        "    formatted_prompt = evaluation_prompt_template.format(article=article_text)\n",
        "\n",
        "    # Send the CoT prompt to Claude\n",
        "    response = client.messages.create(\n",
        "        model=\"claude-3-7-sonnet-20250219\",\n",
        "        max_tokens=500,\n",
        "        temperature=0.5,\n",
        "        messages=[{\"role\": \"user\", \"content\": formatted_prompt}]\n",
        "    )\n",
        "\n",
        "    # Extract sentiment prediction and explanation\n",
        "    response_text = response.content[0].text\n",
        "\n",
        "    try:\n",
        "        claude_sentiment = float(response_text.split(\"Final Answer: \")[-1].strip())\n",
        "    except ValueError:\n",
        "        claude_sentiment = None  # If extraction fails, set to None\n",
        "\n",
        "    claude_explanation = response_text  # Save full explanation\n",
        "\n",
        "    # Store sentiment results\n",
        "    sentiment_results.append({\n",
        "        \"Ticker\": row[\"Ticker\"],\n",
        "        \"Date\": row[\"Date\"],\n",
        "        \"Original Title\": row[\"Original Title\"],\n",
        "        \"Cleaned Title\": row[\"Cleaned Title\"],\n",
        "        \"Sentiment Score\": claude_sentiment\n",
        "    })\n",
        "\n",
        "    # Store sentiment explanation results\n",
        "    explanation_results.append({\n",
        "        \"Ticker\": row[\"Ticker\"],\n",
        "        \"Date\": row[\"Date\"],\n",
        "        \"Original Title\": row[\"Original Title\"],\n",
        "        \"Cleaned Title\": row[\"Cleaned Title\"],\n",
        "        \"Sentiment Explanation\": claude_explanation\n",
        "    })\n",
        "    # Print progress\n",
        "    print(f\"‚úÖ Processed {index + 1}/{len(df_eval_news)} articles\")\n",
        "time.sleep(1)\n",
        "end = time.time()\n",
        "totalClaudetime = (end-begin)\n",
        "\n",
        "# Convert results to DataFrames\n",
        "df_claude_sentiment = pd.DataFrame(sentiment_results)\n",
        "df_claude_explanations = pd.DataFrame(explanation_results)\n",
        "\n",
        "# ‚úÖ Save results to CSV for further analysis\n",
        "df_claude_sentiment.to_csv(\"claude_sentiment_analysis_results_2022_2024.csv\", index=False)\n",
        "df_claude_explanations.to_csv(\"claude_sentiment_explanations_2022_2024.csv\", index=False)\n",
        "#Add graph of reward over time - should be logarithmic to prove training is working\n",
        "print(\"\\n‚úÖ Sentiment analysis results saved to 'claude_sentiment_analysis_results_2022_2024.csv'\")\n",
        "print(\"‚úÖ Sentiment explanations saved to 'claude_sentiment_explanations_2022_2024.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nWY1uyd0zFjo"
      },
      "outputs": [],
      "source": [
        "#Training of OpenAI GPT4 using Chain of Thought prompting and Reinforcement Learning\n",
        "df_news = pd.read_csv(\"historical_financial_news.csv\")\n",
        "df_vader = pd.read_csv(\"vader_sentiment_analysis_results.csv\")\n",
        "\n",
        "# Initialize OpenAI GPT-4 API\n",
        "OPENAI_API_KEY = ''\n",
        "  # Replace with actual API key\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Initialize reward tracking\n",
        "reward_score = 0  # Start with neutral reward score\n",
        "previous_vader = None\n",
        "previous_gpt4 = None\n",
        "begin = time.time()\n",
        "# Chain-of-Thought (CoT) Prompting Template\n",
        "cot_prompt_template = \"\"\"\n",
        "You are an AI trained in financial sentiment analysis.\n",
        "Analyze the sentiment of the following financial news article step by step. Limit your answer in total to 500 characters.\n",
        "\n",
        "1. **Extract Key Phrases**: Identify important financial terms, events, and entities.\n",
        "2. **Determine Sentiment Score**: Provide a sentiment score between -1 (Negative), 0 (Neutral), and +1 (Positive). Please do this to 2 decimal places\n",
        "3. **Explain Sentiment**: Justify the assigned sentiment score based on the financial impact.\n",
        "\n",
        "Example Sentiment Analysis:\n",
        "\n",
        "Article: \"{article}\"\n",
        "---\n",
        "Step 1: Key Phrases ‚Üí [extracted key phrases]\n",
        "Step 2: Sentiment Score ‚Üí [Numeric value between -1 and 1]\n",
        "Step 3: Explanation ‚Üí [detailed reasoning]\n",
        "\n",
        "Final Answer: [Numeric Sentiment Score]\n",
        "\"\"\"\n",
        "\n",
        "# Reinforcement Learning Loop\n",
        "for index, row in df_news.iterrows():\n",
        "    article_text = row[\"Cleaned Title\"]\n",
        "\n",
        "    # Retrieve VADER sentiment score\n",
        "    vader_sentiment = df_vader.loc[df_vader[\"Ticker\"] == row[\"Ticker\"], \"Sentiment Score\"].values[0]\n",
        "\n",
        "    # Update CoT Prompt with Reinforcement Learning Adjustment and Previous Scores\n",
        "    updated_prompt = cot_prompt_template\n",
        "\n",
        "    if reward_score < 0.2 and previous_vader is not None and previous_gpt4 is not None:\n",
        "        updated_prompt += f\"\\n\\n‚ö†Ô∏è Previous predictions have been inaccurate. Pay closer attention to sentiment context.\"\n",
        "        updated_prompt += f\" The last article had a VADER sentiment score of {previous_vader} and you provided {previous_gpt4}.\"\n",
        "    elif reward_score > 0.4 and previous_vader is not None and previous_gpt4 is not None:\n",
        "        updated_prompt += f\"\\n\\n‚úÖ Previous predictions have been accurate. Keep applying the same logic.\"\n",
        "        updated_prompt += f\" The last article had a VADER sentiment score of {previous_vader} and you provided {previous_gpt4}.\"\n",
        "\n",
        "    # Format the prompt with the financial news article\n",
        "    formatted_prompt = updated_prompt.format(article=article_text)\n",
        "\n",
        "    # Send the CoT prompt to GPT-4\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"system\", \"content\": \"You are an expert in financial sentiment analysis.\"},\n",
        "                  {\"role\": \"user\", \"content\": formatted_prompt}],\n",
        "        max_tokens=500,\n",
        "        temperature=0.5\n",
        "    )\n",
        "\n",
        "    # Extract GPT-4 sentiment prediction using string splitting\n",
        "    try:\n",
        "        gpt4_sentiment = float(response[\"choices\"][0][\"message\"][\"content\"].split(\"Final Answer: \")[-1].strip())\n",
        "    except ValueError:\n",
        "        gpt4_sentiment = None  # If extraction fails, set to None\n",
        "\n",
        "    #print(gpt4_sentiment)\n",
        "\n",
        "    #Reward/Penalty Mechanism (Calculate Absolute Difference)\n",
        "    if gpt4_sentiment is not None:\n",
        "        score_difference = abs(gpt4_sentiment - vader_sentiment)  # Closer = better\n",
        "        reward = 1 - score_difference  # Higher reward for smaller difference\n",
        "        reward_score += reward\n",
        "\n",
        "        print(f\"üì∞ Article: {article_text}\")\n",
        "        print(f\"‚úÖ GPT-4 Score: {gpt4_sentiment} | VADER Score: {vader_sentiment} | Reward: {reward:.2f}\")\n",
        "\n",
        "        # Store previous scores for next iteration\n",
        "        previous_vader = vader_sentiment\n",
        "        previous_gpt4 = gpt4_sentiment\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Warning! Could not extract sentiment from GPT-4's response.\")\n",
        "\n",
        "    # Adjust reward tracking\n",
        "    print(f\"üîπ Current Reward Score: {reward_score:.2f}\\n\")\n",
        "\n",
        "print(\"\\n‚úÖ Reinforcement Learning complete! GPT-4 has processed all 100 financial news articles with adaptive sentiment refinement.\")\n",
        "\n",
        "# Evaluation of OpenAI GPT4 after the model has been trained\n",
        "\n",
        "df_evaluation = pd.read_csv(\"evaluation_financial_news.csv\")\n",
        "\n",
        "evaluation_prompt_template = \"\"\"\n",
        "You have earlier been provided with a training dataset consisting of equity based financial articles from 2011-2021.\n",
        "You were asked to perform sentiment analysis on those articles, while being guided as to how accurate your predictions were\n",
        "Now, you will be prompted with a set of financial articles from 2022-2024, and use the information you gained from the training process to do the following:\n",
        "Analyze the sentiment of the following financial news article step by step. Limit your answer in total to 500 characters.\n",
        "\n",
        "1. **Extract Key Phrases**: Identify important financial terms, events, and entities.\n",
        "2. **Determine Sentiment Score**: Provide a sentiment score between -1 (Negative), 0 (Neutral), and +1 (Positive). Please do this to 4 decimal places\n",
        "3. **Explain Sentiment**: Justify the assigned sentiment score based on the financial impact.\n",
        "\n",
        "Example Sentiment Analysis:\n",
        "\n",
        "Article: \"{article}\"\n",
        "---\n",
        "Step 1: Key Phrases ‚Üí [extracted key phrases]\n",
        "Step 2: Sentiment Score ‚Üí [Numeric value between -1 and 1]\n",
        "Step 3: Explanation ‚Üí [detailed reasoning]\n",
        "\n",
        "Final Answer: [Numeric Sentiment Score]\n",
        "\"\"\"\n",
        "sentiment_results = []\n",
        "explanation_results = []\n",
        "\n",
        "for index, row in df_eval_news.iterrows():\n",
        "    article_text = row[\"Cleaned Title\"]\n",
        "\n",
        "    # Format the prompt with the financial news article\n",
        "    formatted_prompt = evaluation_prompt_template.format(article=article_text)\n",
        "\n",
        "    # Send the CoT prompt to GPT-4\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"system\", \"content\": \"You are an expert in financial sentiment analysis.\"},\n",
        "                  {\"role\": \"user\", \"content\": formatted_prompt}],\n",
        "        max_tokens=500,\n",
        "        temperature=0.5\n",
        "    )\n",
        "\n",
        "    # Extract sentiment prediction\n",
        "    try:\n",
        "        gpt_sentiment = float(response[\"choices\"][0][\"message\"][\"content\"].split(\"Final Answer: \")[-1].strip())\n",
        "    except ValueError:\n",
        "        gpt_sentiment = None  # If extraction fails, set to None\n",
        "\n",
        "    gpt_explanation = response_text\n",
        "    # Store results\n",
        "    sentiment_results.append({\n",
        "        \"Ticker\": row[\"Ticker\"],\n",
        "        \"Date\": row[\"Date\"],\n",
        "        \"Original Title\": row[\"Original Title\"],\n",
        "        \"Cleaned Title\": row[\"Cleaned Title\"],\n",
        "        \"Sentiment Score\": gpt_sentiment\n",
        "    })\n",
        "    explanation_results.append({\n",
        "        \"Ticker\": row[\"Ticker\"],\n",
        "        \"Date\": row[\"Date\"],\n",
        "        \"Original Title\": row[\"Original Title\"],\n",
        "        \"Cleaned Title\": row[\"Cleaned Title\"],\n",
        "        \"Sentiment Explanation\": gpt_explanation\n",
        "    })\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"‚úÖ Processed {index + 1}/{len(df_eval_news)} articles\")\n",
        "time.sleep(1)\n",
        "end = time.time()\n",
        "totalGPTtime = (end-begin)\n",
        "df_gpt_sentiment = pd.DataFrame(sentiment_results)\n",
        "df_gpt_explanations = pd.DataFrame(explanation_results)\n",
        "\n",
        "df_gpt_sentiment.to_csv(\"gpt_sentiment_analysis_results_2022_2024.csv\", index=False)\n",
        "df_gpt_explanations.to_csv(\"gpt_sentiment_explanations_2022_2024.csv\", index=False)\n",
        "\n",
        "print(\"\\n‚úÖ Sentiment analysis results saved to 'gpt_sentiment_analysis_results_2022_2024.csv'\")\n",
        "print(\"‚úÖ Sentiment explanations saved to 'gpt_sentiment_explanations_2022_2024.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vd6gMx0UGyf6"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "HF_TOKEN=userdata.get('test_token')\n",
        "\n",
        "if HF_TOKEN:\n",
        "    login(HF_TOKEN)\n",
        "    print(\"Successfully logged in to Hugging Face!\")\n",
        "else:\n",
        "    print(\"Token is not set. Please save the token first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "p_mZKs8hHZFS"
      },
      "outputs": [],
      "source": [
        "#Training Meta LLaMa - only do this if it works\n",
        "'''\n",
        "# üîπ Load the 100 preprocessed financial news articles and VADER sentiment results\n",
        "df_news = pd.read_csv(\"historical_financial_news.csv\")\n",
        "df_vader = pd.read_csv(\"vader_sentiment_analysis_results.csv\")\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "begin = time.time()\n",
        "# üîπ Initialize reward tracking\n",
        "reward_score = 0  # Start with neutral reward score\n",
        "previous_vader = None\n",
        "previous_llama = None\n",
        "\n",
        "# üîπ Chain-of-Thought (CoT) Prompting Template\n",
        "cot_prompt_template = \"\"\"\n",
        "You are an AI trained in financial sentiment analysis.\n",
        "Analyze the sentiment of the following financial news article step by step. Limit your answer in total to 500 characters.\n",
        "\n",
        "1. **Extract Key Phrases**: Identify important financial terms, events, and entities.\n",
        "2. **Determine Sentiment Score**: Provide a sentiment score between -1 (Negative), 0 (Neutral), and +1 (Positive). Please do this to 2 decimal places\n",
        "3. **Explain Sentiment**: Justify the assigned sentiment score based on the financial impact.\n",
        "\n",
        "Example Sentiment Analysis:\n",
        "\n",
        "Article: \"{article}\"\n",
        "---\n",
        "Step 1: Key Phrases ‚Üí [extracted key phrases]\n",
        "Step 2: Sentiment Score ‚Üí [Numeric value between -1 and 1]\n",
        "Step 3: Explanation ‚Üí [detailed reasoning]\n",
        "\n",
        "Final Answer: [Numeric Sentiment Score]\n",
        "\"\"\"\n",
        "\n",
        "# üîπ Reinforcement Learning Loop\n",
        "for index, row in df_news.iterrows():\n",
        "    article_text = row[\"Cleaned Title\"]\n",
        "\n",
        "    # Retrieve VADER sentiment score\n",
        "    vader_sentiment = df_vader.loc[df_vader[\"Ticker\"] == row[\"Ticker\"], \"Sentiment Score\"].values[0]\n",
        "\n",
        "    # üîπ Update CoT Prompt with Reinforcement Learning Adjustment and Previous Scores\n",
        "    updated_prompt = cot_prompt_template\n",
        "\n",
        "    if reward_score < 0.2 and previous_vader is not None and previous_llama is not None:\n",
        "        updated_prompt += f\"\\n\\n‚ö†Ô∏è Previous predictions have been inaccurate. Pay closer attention to sentiment context.\"\n",
        "        updated_prompt += f\" The last article had a VADER sentiment score of {previous_vader} and you provided {previous_llama}.\"\n",
        "    elif reward_score > 0.4 and previous_vader is not None and previous_llama is not None:\n",
        "        updated_prompt += f\"\\n\\n‚úÖ Previous predictions have been accurate. Keep applying the same logic.\"\n",
        "        updated_prompt += f\" The last article had a VADER sentiment score of {previous_vader} and you provided {previous_llama}.\"\n",
        "\n",
        "    formatted_prompt = updated_prompt.format(article=article_text)\n",
        "\n",
        "    # Generate response from LLaMA\n",
        "    response = pipe(formatted_prompt, max_new_tokens=200)\n",
        "    response_text = response[0][\"generated_text\"]\n",
        "\n",
        "    # Extract LLaMA sentiment prediction\n",
        "    match = re.search(r\"Final Answer: (-?\\d+\\.\\d+)\", response_text)\n",
        "    llama_sentiment = float(match.group(1)) if match else None\n",
        "\n",
        "    # üîπ Reward/Penalty Mechanism (Calculate Absolute Difference)\n",
        "    if llama_sentiment is not None:\n",
        "        score_difference = abs(llama_sentiment - vader_sentiment)  # Closer = better\n",
        "        reward = 1 - score_difference  # Higher reward for smaller difference\n",
        "        reward_score += reward\n",
        "\n",
        "        print(f\"üì∞ Article: {article_text}\")\n",
        "        print(f\"‚úÖ LLaMA Score: {llama_sentiment} | VADER Score: {vader_sentiment} | Reward: {reward:.2f}\")\n",
        "\n",
        "        # Store previous scores for next iteration\n",
        "        previous_vader = vader_sentiment\n",
        "        previous_llama = llama_sentiment\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Warning! Could not extract sentiment from LLaMA's response.\")\n",
        "\n",
        "    # Adjust reward tracking\n",
        "    print(f\"üîπ Current Reward Score: {reward_score:.2f}\\n\")\n",
        "\n",
        "print(\"\\n‚úÖ Reinforcement Learning complete! LLaMA has processed all 100 financial news articles with adaptive sentiment refinement.\")\n",
        "\n",
        "df_eval_news = pd.read_csv(\"evaluation_financial_news.csv\")\n",
        "\n",
        "evaluation_prompt_template = \"\"\"\n",
        "You have earlier been provided with a training dataset consisting of equity based financial articles from 2011-2021.\n",
        "You were asked to perform sentiment analysis on those articles, while being guided as to how accurate your predictions were\n",
        "Now, you will be prompted with a set of financial articles from 2022-2024, and use the information you gained from the training process to do the following:\n",
        "Analyze the sentiment of the following financial news article step by step. Limit your answer in total to 500 characters.\n",
        "\n",
        "1. **Extract Key Phrases**: Identify important financial terms, events, and entities.\n",
        "2. **Determine Sentiment Score**: Provide a sentiment score between -1 (Negative), 0 (Neutral), and +1 (Positive). Please do this to 4 decimal places\n",
        "3. **Explain Sentiment**: Justify the assigned sentiment score based on the financial impact.\n",
        "\n",
        "Example Sentiment Analysis:\n",
        "\n",
        "Article: \"{article}\"\n",
        "---\n",
        "Step 1: Key Phrases ‚Üí [extracted key phrases]\n",
        "Step 2: Sentiment Score ‚Üí [Numeric value between -1 and 1]\n",
        "Step 3: Explanation ‚Üí [detailed reasoning]\n",
        "\n",
        "Final Answer: [Numeric Sentiment Score]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "sentiment_results = []\n",
        "\n",
        "for index, row in df_eval_news.iterrows():\n",
        "    article_text = row[\"Cleaned Title\"]\n",
        "\n",
        "    # Format the prompt with the financial news article\n",
        "    formatted_prompt = evaluation_prompt_template.format(article=article_text)\n",
        "\n",
        "    response = pipe(formatted_prompt, max_new_tokens=200)\n",
        "    response_text = response[0][\"generated_text\"]\n",
        "\n",
        "    # Extract LLaMA sentiment prediction\n",
        "    match = re.search(r\"Final Answer: (-?\\d+\\.\\d+)\", response_text)\n",
        "    llama_sentiment = float(match.group(1)) if match else None\n",
        "\n",
        "    # Extract LLaMA sentiment prediction using string splitting\n",
        "    try:\n",
        "        llama_sentiment = float(response_text.split(\"Final Answer: \")[-1].strip())\n",
        "    except ValueError:\n",
        "        llama_sentiment = None  # If extraction fails, set to None\n",
        "\n",
        "    # Store results\n",
        "    sentiment_results.append({\n",
        "        \"Ticker\": row[\"Ticker\"],\n",
        "        \"Date\": row[\"Date\"],\n",
        "        \"Original Title\": row[\"Original Title\"],\n",
        "        \"Cleaned Title\": row[\"Cleaned Title\"],\n",
        "        \"Sentiment Score\": llama_sentiment\n",
        "    })\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"‚úÖ Processed {index + 1}/{len(df_eval_news)} articles\")\n",
        "time.sleep(1)\n",
        "end = time.time()\n",
        "totalLlamatime = (end-begin)\n",
        "# Convert results to DataFrame\n",
        "df_llama_sentiment = pd.DataFrame(sentiment_results)\n",
        "\n",
        "# ‚úÖ Save results to CSV for further analysis\n",
        "df_llama_sentiment.to_csv(\"llama_sentiment_analysis_results_2022_2024.csv\", index=False)\n",
        "\n",
        "print(\"\\n‚úÖ Sentiment analysis results saved to 'llama_sentiment_analysis_results_2022_2024.csv'\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fHUepjeGKE2D"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Training of Google Gemini using Chain of Thought and Reinforcement Learning - only pay and use if meta LLaMa doesn't work\n",
        "\n",
        "df_news = pd.read_csv(\"historical_financial_news.csv\")\n",
        "df_vader = pd.read_csv(\"vader_sentiment_analysis_results.csv\")\n",
        "\n",
        "# Initialize Google Gemini API\n",
        "\n",
        "GEMINI_API_KEY = \"\"  # Replace with actual API key\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "begin = time.time()\n",
        "\n",
        "# Load Gemini model\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# Initialize reward tracking\n",
        "reward_score = 0  # Start with neutral reward score\n",
        "previous_vader = None\n",
        "previous_gemini = None\n",
        "\n",
        "# Chain-of-Thought (CoT) Prompting Template\n",
        "cot_prompt_template = \"\"\"\n",
        "You are an AI trained in financial sentiment analysis.\n",
        "Analyze the sentiment of the following financial news article step by step. Limit your answer in total to 500 characters.\n",
        "Article: \"{article}\"\n",
        "\n",
        "1. **Extract Key Phrases**: Identify important financial terms, events, and entities.\n",
        "2. **Determine Sentiment Score**: Provide a sentiment score between -1 (Negative), 0 (Neutral), and +1 (Positive). Please do this to 2 decimal places\n",
        "3. **Explain Sentiment**: Justify the assigned sentiment score based on the financial impact.\n",
        "\n",
        "Example Sentiment Analysis:\n",
        "\n",
        "Article: \"{article}\"\n",
        "---\n",
        "Step 1: Key Phrases ‚Üí [extracted key phrases]\n",
        "Step 2: Sentiment Score ‚Üí [Numeric value between -1 and 1]\n",
        "Step 3: Explanation ‚Üí [detailed reasoning]\n",
        "\n",
        "Final Answer: [Numeric Sentiment Score]\n",
        "\"\"\"\n",
        "reward_scores_list = []\n",
        "# Reinforcement Learning Loop\n",
        "for index, row in df_news.iterrows():\n",
        "    article_text = row[\"Cleaned Title\"]\n",
        "\n",
        "    # Retrieve VADER sentiment score\n",
        "    vader_sentiment = df_vader.loc[df_vader[\"Ticker\"] == row[\"Ticker\"], \"Sentiment Score\"].values[0]\n",
        "\n",
        "    # Update CoT Prompt with Reinforcement Learning Adjustment and Previous Scores\n",
        "    updated_prompt = cot_prompt_template\n",
        "\n",
        "    if reward_score < 0.2 and previous_vader is not None and previous_gemini is not None:\n",
        "        updated_prompt += f\"\\n\\n‚ö†Ô∏è Previous predictions have been inaccurate. Pay closer attention to sentiment context.\"\n",
        "        updated_prompt += f\" The last article had a VADER sentiment score of {previous_vader} and you provided {previous_gemini}.\"\n",
        "    elif reward_score > 0.4 and previous_vader is not None and previous_gemini is not None:\n",
        "        updated_prompt += f\"\\n\\n‚úÖ Previous predictions have been accurate. Keep applying the same logic.\"\n",
        "        updated_prompt += f\" The last article had a VADER sentiment score of {previous_vader} and you provided {previous_gemini}.\"\n",
        "    print(\"Article text = \" + article_text)\n",
        "    # Format the prompt with the financial news article\n",
        "    formatted_prompt = updated_prompt.format(article=article_text)\n",
        "\n",
        "    # Generate response from Gemini\n",
        "    response = model.generate_content(formatted_prompt)\n",
        "    #print(response)\n",
        "    # Extract Gemini sentiment prediction using string splitting\n",
        "    try:\n",
        "        gemini_sentiment = float(response.text.split(\"Final Answer: \")[-1].strip())\n",
        "    except ValueError:\n",
        "        gemini_sentiment = None  # If extraction fails, set to None\n",
        "\n",
        "    # Reward/Penalty Mechanism (Calculate Absolute Difference)\n",
        "    if gemini_sentiment is not None:\n",
        "        score_difference = abs(gemini_sentiment - vader_sentiment)  # Closer = better\n",
        "        reward = 1 - score_difference  # Higher reward for smaller difference\n",
        "        reward_score += reward\n",
        "\n",
        "        reward_scores_list.append({\n",
        "        \"Ticker\": ticker,\n",
        "        \"Date\": date,\n",
        "        \"Reward Score\": reward_score\n",
        "        })\n",
        "\n",
        "        print(f\"üì∞ Article: {article_text}\")\n",
        "        print(f\"‚úÖ Gemini Score: {gemini_sentiment} | VADER Score: {vader_sentiment} | Reward: {reward:.2f}\")\n",
        "\n",
        "        # Store previous scores for next iteration\n",
        "        previous_vader = vader_sentiment\n",
        "        previous_gemini = gemini_sentiment\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è Warning! Could not extract sentiment from Gemini's response.\")\n",
        "\n",
        "    # Adjust reward tracking\n",
        "    print(f\"Current Reward Score: {reward_score:.2f}\\n\")\n",
        "\n",
        "print(\"\\n‚úÖ Reinforcement Learning complete! Gemini has processed all 100 financial news articles with adaptive sentiment refinement.\")\n",
        "\n",
        "\n",
        "df_eval_news = pd.read_csv(\"evaluation_financial_news.csv\")\n",
        "\n",
        "\n",
        "evaluation_prompt_template = \"\"\"\n",
        "You have earlier been provided with a training dataset consisting of equity based financial articles from 2011-2021.\n",
        "You were asked to perform sentiment analysis on those articles, while being guided as to how accurate your predictions were\n",
        "Now, you will be prompted with a set of financial articles from 2022-2024, and use the information you gained from the training process to do the following:\n",
        "Analyze the sentiment of the following financial news article step by step. Limit your answer in total to 500 characters.\n",
        "Article: \"{article}\"\n",
        "\n",
        "1. **Extract Key Phrases**: Identify important financial terms, events, and entities.\n",
        "2. **Determine Sentiment Score**: Provide a sentiment score between -1 (Negative), 0 (Neutral), and +1 (Positive). Please do this to 4 decimal places\n",
        "3. **Explain Sentiment**: Justify the assigned sentiment score based on the financial impact.\n",
        "\n",
        "Example Sentiment Analysis:\n",
        "\n",
        "Article: \"{article}\"\n",
        "---\n",
        "Step 1: Key Phrases ‚Üí [extracted key phrases]\n",
        "Step 2: Sentiment Score ‚Üí [Numeric value between -1 and 1]\n",
        "Step 3: Explanation ‚Üí [detailed reasoning]\n",
        "\n",
        "Final Answer: [Numeric Sentiment Score]\n",
        "\"\"\"\n",
        "sentiment_results = []\n",
        "explanation_results = []\n",
        "\n",
        "for index, row in df_eval_news.iterrows():\n",
        "    article_text = row[\"Cleaned Title\"]\n",
        "\n",
        "    # Format the prompt with the financial news article\n",
        "    formatted_prompt = evaluation_prompt_template.format(article=article_text)\n",
        "\n",
        "    # üîπ Generate response from Gemini\n",
        "    response = model.generate_content(formatted_prompt)\n",
        "\n",
        "    # Extract sentiment prediction and explanation\n",
        "    response_text = response.text\n",
        "\n",
        "    try:\n",
        "        gemini_sentiment = float(response_text.split(\"Final Answer: \")[-1].strip())\n",
        "    except ValueError:\n",
        "        gemini_sentiment = None  # If extraction fails, set to None\n",
        "\n",
        "    gemini_explanation = response_text  # Save full explanation\n",
        "    print(gemini_explanation)\n",
        "    # Store sentiment results\n",
        "    sentiment_results.append({\n",
        "        \"Ticker\": row[\"Ticker\"],\n",
        "        \"Date\": row[\"Date\"],\n",
        "        \"Original Title\": row[\"Original Title\"],\n",
        "        \"Cleaned Title\": row[\"Cleaned Title\"],\n",
        "        \"Sentiment Score\": gemini_sentiment\n",
        "    })\n",
        "\n",
        "    # Store sentiment explanation results\n",
        "    explanation_results.append({\n",
        "        \"Ticker\": row[\"Ticker\"],\n",
        "        \"Date\": row[\"Date\"],\n",
        "        \"Original Title\": row[\"Original Title\"],\n",
        "        \"Cleaned Title\": row[\"Cleaned Title\"],\n",
        "        \"Sentiment Explanation\": gemini_explanation\n",
        "    })\n",
        "\n",
        "    # Print progress\n",
        "    print(f\"‚úÖ Processed {index + 1}/{len(df_eval_news)} articles\")\n",
        "time.sleep(1)\n",
        "end = time.time()\n",
        "totalGeminitime = (end-begin)\n",
        "# Convert results to DataFrames\n",
        "df_gemini_sentiment = pd.DataFrame(sentiment_results)\n",
        "df_gemini_explanations = pd.DataFrame(explanation_results)\n",
        "df_rewards = pd.DataFrame(reward_scores_list)\n",
        "df_rewards.to_csv(\"gemini_reward_scores_training.csv\", index=False)\n",
        "print(\"\\n‚úÖ Reward scores saved to 'gemini_reward_scores_training.csv'\")\n",
        "# ‚úÖ Save results to CSV for further analysis\n",
        "df_gemini_sentiment.to_csv(\"gemini_sentiment_analysis_results_2022_2024.csv\", index=False)\n",
        "df_gemini_explanations.to_csv(\"gemini_sentiment_explanations_2022_2024.csv\", index=False)\n",
        "\n",
        "print(\"\\n‚úÖ Sentiment analysis results saved to 'gemini_sentiment_analysis_results_2022_2024.csv'\")\n",
        "print(\"‚úÖ Sentiment explanations saved to 'gemini_sentiment_explanations_2022_2024.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_PtA8iggLigU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# üîπ Load sentiment explanation datasets\n",
        "df_claude = pd.read_csv(\"claude_sentiment_explanations_2022_2024.csv\")\n",
        "df_gpt = pd.read_csv(\"gpt_sentiment_explanations_2022_2024.csv\")\n",
        "df_gemini = pd.read_csv(\"gemini_sentiment_explanations_2022_2024.csv\")\n",
        "\n",
        "# üîπ Merge datasets by Ticker and Date to align explanations\n",
        "df_merged = pd.merge(df_claude, df_gpt, on=[\"Ticker\", \"Date\", \"Original Title\", \"Cleaned Title\"], suffixes=(\"_claude\", \"_gpt\"))\n",
        "df_merged = pd.merge(df_merged, df_gemini, on=[\"Ticker\", \"Date\", \"Original Title\", \"Cleaned Title\"])\n",
        "df_merged.rename(columns={\"Sentiment Explanation\": \"Sentiment Explanation_gemini\"}, inplace=True)\n",
        "\n",
        "# üîπ Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "\n",
        "# üîπ Compute ROUGE F1 scores for each explanation pair\n",
        "rouge_scores = []\n",
        "\n",
        "for index, row in df_merged.iterrows():\n",
        "    claude_text = row[\"Sentiment Explanation_claude\"]\n",
        "    gpt_text = row[\"Sentiment Explanation_gpt\"]\n",
        "    gemini_text = row[\"Sentiment Explanation_gemini\"]\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    scores_claude_gpt = scorer.score(claude_text, gpt_text)\n",
        "    scores_claude_gemini = scorer.score(claude_text, gemini_text)\n",
        "    scores_gpt_gemini = scorer.score(gpt_text, gemini_text)\n",
        "\n",
        "    rouge_scores.append({\n",
        "        \"Ticker\": row[\"Ticker\"],\n",
        "        \"Date\": row[\"Date\"],\n",
        "        \"ROUGE-1 Claude vs GPT\": scores_claude_gpt[\"rouge1\"].fmeasure,\n",
        "        \"ROUGE-2 Claude vs GPT\": scores_claude_gpt[\"rouge2\"].fmeasure,\n",
        "        \"ROUGE-L Claude vs GPT\": scores_claude_gpt[\"rougeL\"].fmeasure,\n",
        "        \"ROUGE-1 Claude vs Gemini\": scores_claude_gemini[\"rouge1\"].fmeasure,\n",
        "        \"ROUGE-2 Claude vs Gemini\": scores_claude_gemini[\"rouge2\"].fmeasure,\n",
        "        \"ROUGE-L Claude vs Gemini\": scores_claude_gemini[\"rougeL\"].fmeasure,\n",
        "        \"ROUGE-1 GPT vs Gemini\": scores_gpt_gemini[\"rouge1\"].fmeasure,\n",
        "        \"ROUGE-2 GPT vs Gemini\": scores_gpt_gemini[\"rouge2\"].fmeasure,\n",
        "        \"ROUGE-L GPT vs Gemini\": scores_gpt_gemini[\"rougeL\"].fmeasure,\n",
        "    })\n",
        "\n",
        "# üîπ Convert to DataFrame\n",
        "df_rouge_results = pd.DataFrame(rouge_scores)\n",
        "\n",
        "# ‚úÖ Save results to CSV\n",
        "df_rouge_results.to_csv(\"rouge_f1_comparison_claude_gpt_gemini.csv\", index=False)\n",
        "\n",
        "print(\"\\n‚úÖ ROUGE F1 comparison results saved to 'rouge_f1_comparison_claude_gpt_gemini.csv'\")\n",
        "\n",
        "# üìå Display summary statistics\n",
        "print(\"\\nüìå ROUGE F1 Score Summary:\")\n",
        "print(df_rouge_results.describe())\n",
        "\n",
        "# üîπ Plot ROUGE-1 Differences\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(df_rouge_results[\"ROUGE-1 Claude vs GPT\"], label=\"ROUGE-1 Claude vs GPT\", linestyle='-', marker='o', alpha=0.7)\n",
        "plt.plot(df_rouge_results[\"ROUGE-1 Claude vs Gemini\"], label=\"ROUGE-1 Claude vs Gemini\", linestyle='-', marker='s', alpha=0.7)\n",
        "plt.plot(df_rouge_results[\"ROUGE-1 GPT vs Gemini\"], label=\"ROUGE-1 GPT vs Gemini\", linestyle='-', marker='d', alpha=0.7)\n",
        "\n",
        "plt.xlabel(\"Articles\")\n",
        "plt.ylabel(\"ROUGE-1 F1 Score\")\n",
        "plt.title(\"ROUGE-1 F1 Score Differences Between Claude, GPT-4, and Gemini\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# üîπ Plot ROUGE-2 Differences\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(df_rouge_results[\"ROUGE-2 Claude vs GPT\"], label=\"ROUGE-2 Claude vs GPT\", linestyle='-', marker='o', alpha=0.7)\n",
        "plt.plot(df_rouge_results[\"ROUGE-2 Claude vs Gemini\"], label=\"ROUGE-2 Claude vs Gemini\", linestyle='-', marker='s', alpha=0.7)\n",
        "plt.plot(df_rouge_results[\"ROUGE-2 GPT vs Gemini\"], label=\"ROUGE-2 GPT vs Gemini\", linestyle='-', marker='d', alpha=0.7)\n",
        "\n",
        "plt.xlabel(\"Articles\")\n",
        "plt.ylabel(\"ROUGE-2 F1 Score\")\n",
        "plt.title(\"ROUGE-2 F1 Score Differences Between Claude, GPT-4, and Gemini\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# üîπ Plot ROUGE-L Differences\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(df_rouge_results[\"ROUGE-L Claude vs GPT\"], label=\"ROUGE-L Claude vs GPT\", linestyle='-', marker='o', alpha=0.7)\n",
        "plt.plot(df_rouge_results[\"ROUGE-L Claude vs Gemini\"], label=\"ROUGE-L Claude vs Gemini\", linestyle='-', marker='s', alpha=0.7)\n",
        "plt.plot(df_rouge_results[\"ROUGE-L GPT vs Gemini\"], label=\"ROUGE-L GPT vs Gemini\", linestyle='-', marker='d', alpha=0.7)\n",
        "\n",
        "plt.xlabel(\"Articles\")\n",
        "plt.ylabel(\"ROUGE-L F1 Score\")\n",
        "plt.title(\"ROUGE-L F1 Score Differences Between Claude, GPT-4, and Gemini\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bys0zCXMTzCQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy.stats import pearsonr\n",
        "from rouge_score import rouge_scorer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kllsAIwfT4Gw"
      },
      "outputs": [],
      "source": [
        "# üîπ Load sentiment analysis results for Claude and VADER\n",
        "df_vader = pd.read_csv(\"vader_sentiment_analysis_results_2022_2024.csv\")\n",
        "df_claude = pd.read_csv(\"claude_sentiment_analysis_results_2022_2024.csv\")\n",
        "df_claude_explanations = pd.read_csv(\"claude_sentiment_explanations_2022_2024.csv\")\n",
        "\n",
        "# üîπ Merge datasets on Ticker and Date to align sentiment scores\n",
        "df_merged = df_vader.merge(df_claude, on=[\"Ticker\", \"Date\", \"Original Title\", \"Cleaned Title\"], suffixes=(\"_vader\", \"_claude\"))\n",
        "\n",
        "# üîπ Extract sentiment scores\n",
        "vader_scores = df_merged[\"Sentiment Score_vader\"].values\n",
        "claude_scores = df_merged[\"Sentiment Score_claude\"].values\n",
        "\n",
        "# üîπ Replace NaNs and infinite values with finite numbers\n",
        "vader_scores = np.nan_to_num(vader_scores, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "claude_scores = np.nan_to_num(claude_scores, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "\n",
        "# üîπ Compute Pearson Correlation Coefficient safely\n",
        "if len(vader_scores) > 1 and len(claude_scores) > 1:\n",
        "    pearson_claude, _ = pearsonr(vader_scores, claude_scores)\n",
        "else:\n",
        "    pearson_claude = np.nan  # Assign NaN if correlation cannot be computed\n",
        "\n",
        "# üîπ Compute Mean Squared Error (MSE)\n",
        "mse_claude = mean_squared_error(vader_scores, claude_scores)\n",
        "\n",
        "# üîπ Compute ROUGE F1 scores for explanations\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "rouge_scores = []\n",
        "\n",
        "for index, row in df_claude_explanations.iterrows():\n",
        "    reference_text = row[\"Sentiment Explanation\"]\n",
        "    candidate_text = row[\"Sentiment Explanation\"]  # Compare Claude to itself for structure\n",
        "\n",
        "    scores = scorer.score(reference_text, candidate_text)\n",
        "\n",
        "    rouge_scores.append({\n",
        "        \"Ticker\": row[\"Ticker\"],\n",
        "        \"Date\": row[\"Date\"],\n",
        "        \"ROUGE-1 F1\": scores[\"rouge1\"].fmeasure if scores[\"rouge1\"].fmeasure is not None else 0.0,\n",
        "        \"ROUGE-2 F1\": scores[\"rouge2\"].fmeasure if scores[\"rouge2\"].fmeasure is not None else 0.0,\n",
        "        \"ROUGE-L F1\": scores[\"rougeL\"].fmeasure if scores[\"rougeL\"].fmeasure is not None else 0.0,\n",
        "    })\n",
        "\n",
        "df_rouge_results = pd.DataFrame(rouge_scores)\n",
        "\n",
        "# üîπ Replace NaNs/Infs in ROUGE scores\n",
        "df_rouge_results.fillna(0, inplace=True)\n",
        "\n",
        "# üîπ Calculate average ROUGE F1 scores\n",
        "rouge1_f1_avg = df_rouge_results[\"ROUGE-1 F1\"].mean()\n",
        "rouge2_f1_avg = df_rouge_results[\"ROUGE-2 F1\"].mean()\n",
        "rougeL_f1_avg = df_rouge_results[\"ROUGE-L F1\"].mean()\n",
        "\n",
        "# üìä Store evaluation results\n",
        "evaluation_results_claude = pd.DataFrame({\n",
        "    \"Metric\": [\"MSE\", \"Pearson Correlation\", \"ROUGE-1 F1\", \"ROUGE-2 F1\", \"ROUGE-L F1\"],\n",
        "    \"Claude vs VADER\": [mse_claude, pearson_claude, rouge1_f1_avg, rouge2_f1_avg, rougeL_f1_avg]\n",
        "})\n",
        "\n",
        "print(\"MSE: \" + str(mse_claude))\n",
        "print(\"Pearson: \" + str(pearson_claude))\n",
        "\n",
        "# ‚úÖ Save results to CSV\n",
        "evaluation_results_claude.to_csv(\"evaluation_metrics_claude_vs_vader.csv\", index=False)\n",
        "print(\"\\n‚úÖ Evaluation results saved to 'evaluation_metrics_claude_vs_vader.csv'\")\n",
        "\n",
        "# üîπ Plot comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "metrics = evaluation_results_claude[\"Metric\"]\n",
        "values = evaluation_results_claude[\"Claude vs VADER\"]\n",
        "\n",
        "plt.barh(metrics, values, alpha=0.7, color=\"blue\", label=\"Claude vs VADER\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Metric\")\n",
        "plt.title(\"Claude vs VADER: MSE, Pearson R, ROUGE F1 (Handling NaNs/Infs)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# üìä Show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AUAbOzfhmqK2"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# üîπ Load sentiment analysis results for GPT-4 and VADER\n",
        "df_vader = pd.read_csv(\"vader_sentiment_analysis_results_2022_2024.csv\")\n",
        "df_gpt = pd.read_csv(\"gpt_sentiment_analysis_results_2022_2024.csv\")\n",
        "df_gpt_explanations = pd.read_csv(\"gpt_sentiment_explanations_2022_2024.csv\")\n",
        "\n",
        "# üîπ Merge datasets on Ticker and Date to align sentiment scores\n",
        "df_merged = df_vader.merge(df_gpt, on=[\"Ticker\", \"Date\", \"Original Title\", \"Cleaned Title\"], suffixes=(\"_vader\", \"_gpt\"))\n",
        "\n",
        "# üîπ Extract sentiment scores\n",
        "vader_scores = df_merged[\"Sentiment Score_vader\"].values\n",
        "gpt_scores = df_merged[\"Sentiment Score_gpt\"].values\n",
        "\n",
        "# üîπ Replace NaNs and infinite values with finite numbers\n",
        "vader_scores = np.nan_to_num(vader_scores, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "gpt_scores = np.nan_to_num(gpt_scores, nan=0.0, posinf=1.0, neginf=-1.0)\n",
        "\n",
        "# üîπ Compute Pearson Correlation Coefficient safely\n",
        "if len(vader_scores) > 1 and len(gpt_scores) > 1:\n",
        "    pearson_gpt, _ = pearsonr(vader_scores, gpt_scores)\n",
        "else:\n",
        "    pearson_gpt = np.nan  # Assign NaN if correlation cannot be computed\n",
        "\n",
        "# üîπ Compute Mean Squared Error (MSE)\n",
        "mse_gpt = mean_squared_error(vader_scores, gpt_scores)\n",
        "\n",
        "# üîπ Compute ROUGE F1 scores for explanations\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "rouge_scores = []\n",
        "\n",
        "for index, row in df_gpt_explanations.iterrows():\n",
        "    reference_text = row[\"Sentiment Explanation\"]\n",
        "    candidate_text = row[\"Sentiment Explanation\"]  # Compare GPT to itself for structure\n",
        "\n",
        "    scores = scorer.score(reference_text, candidate_text)\n",
        "\n",
        "    rouge_scores.append({\n",
        "        \"Ticker\": row[\"Ticker\"],\n",
        "        \"Date\": row[\"Date\"],\n",
        "        \"ROUGE-1 F1\": scores[\"rouge1\"].fmeasure if scores[\"rouge1\"].fmeasure is not None else 0.0,\n",
        "        \"ROUGE-2 F1\": scores[\"rouge2\"].fmeasure if scores[\"rouge2\"].fmeasure is not None else 0.0,\n",
        "        \"ROUGE-L F1\": scores[\"rougeL\"].fmeasure if scores[\"rougeL\"].fmeasure is not None else 0.0,\n",
        "    })\n",
        "\n",
        "df_rouge_results = pd.DataFrame(rouge_scores)\n",
        "\n",
        "# üîπ Replace NaNs/Infs in ROUGE scores\n",
        "df_rouge_results.fillna(0, inplace=True)\n",
        "\n",
        "# üîπ Calculate average ROUGE F1 scores\n",
        "rouge1_f1_avg = df_rouge_results[\"ROUGE-1 F1\"].mean()\n",
        "rouge2_f1_avg = df_rouge_results[\"ROUGE-2 F1\"].mean()\n",
        "rougeL_f1_avg = df_rouge_results[\"ROUGE-L F1\"].mean()\n",
        "\n",
        "# üìä Store evaluation results\n",
        "evaluation_results_gpt = pd.DataFrame({\n",
        "    \"Metric\": [\"MSE\", \"Pearson Correlation\", \"ROUGE-1 F1\", \"ROUGE-2 F1\", \"ROUGE-L F1\"],\n",
        "    \"GPT-4 vs VADER\": [mse_gpt, pearson_gpt, rouge1_f1_avg, rouge2_f1_avg, rougeL_f1_avg]\n",
        "})\n",
        "\n",
        "print(\"MSE: \" + str(mse_gpt))\n",
        "print(\"Pearson: \" + str(pearson_gpt))\n",
        "\n",
        "# ‚úÖ Save results to CSV\n",
        "evaluation_results_gpt.to_csv(\"evaluation_metrics_gpt_vs_vader.csv\", index=False)\n",
        "print(\"\\n‚úÖ Evaluation results saved to 'evaluation_metrics_gpt_vs_vader.csv'\")\n",
        "\n",
        "# üîπ Plot comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "metrics = evaluation_results_gpt[\"Metric\"]\n",
        "values = evaluation_results_gpt[\"GPT-4 vs VADER\"]\n",
        "\n",
        "plt.barh(metrics, values, alpha=0.7, color=\"green\", label=\"GPT-4 vs VADER\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Metric\")\n",
        "plt.title(\"GPT-4 vs VADER: MSE, Pearson R, ROUGE F1 (Handling NaNs/Infs)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# üìä Show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UdOGfC2Gmvif"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# üîπ Load sentiment analysis results for Gemini and VADER\n",
        "df_vader = pd.read_csv(\"vader_sentiment_analysis_results_2022_2024.csv\")\n",
        "df_gemini = pd.read_csv(\"gemini_sentiment_analysis_results_2022_2024.csv\")\n",
        "df_gemini_explanations = pd.read_csv(\"gemini_sentiment_explanations_2022_2024.csv\")\n",
        "\n",
        "# üîπ Merge datasets on Ticker and Date to align sentiment scores\n",
        "df_merged = df_vader.merge(df_gemini, on=[\"Ticker\", \"Date\", \"Original Title\", \"Cleaned Title\"], suffixes=(\"_vader\", \"_gemini\"))\n",
        "\n",
        "# üîπ Extract sentiment scores\n",
        "vader_scores = df_merged[\"Sentiment Score_vader\"]\n",
        "gemini_scores = df_merged[\"Sentiment Score_gemini\"]\n",
        "\n",
        "# üîπ Handle NaN values: Drop rows where either VADER or Gemini score is NaN\n",
        "valid_indices = ~np.isnan(gemini_scores) & ~np.isnan(vader_scores)\n",
        "vader_scores = vader_scores[valid_indices]\n",
        "gemini_scores = gemini_scores[valid_indices]\n",
        "\n",
        "# üîπ Compute Pearson Correlation Coefficient\n",
        "if len(vader_scores) > 1 and len(gemini_scores) > 1:\n",
        "    pearson_gemini, _ = pearsonr(vader_scores, gemini_scores)\n",
        "else:\n",
        "    pearson_gemini = np.nan  # Assign NaN if correlation cannot be computed\n",
        "\n",
        "# üîπ Compute Mean Squared Error (MSE)\n",
        "mse_gemini = mean_squared_error(vader_scores, gemini_scores)\n",
        "\n",
        "# üîπ Compute ROUGE F1 scores for explanations\n",
        "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "rouge_scores = []\n",
        "\n",
        "for index, row in df_gemini_explanations.iterrows():\n",
        "    reference_text = row[\"Sentiment Explanation\"]\n",
        "    candidate_text = row[\"Sentiment Explanation\"]  # Compare Gemini to itself for structure\n",
        "\n",
        "    scores = scorer.score(reference_text, candidate_text)\n",
        "\n",
        "    rouge_scores.append({\n",
        "        \"Ticker\": row[\"Ticker\"],\n",
        "        \"Date\": row[\"Date\"],\n",
        "        \"ROUGE-1 F1\": scores[\"rouge1\"].fmeasure,\n",
        "        \"ROUGE-2 F1\": scores[\"rouge2\"].fmeasure,\n",
        "        \"ROUGE-L F1\": scores[\"rougeL\"].fmeasure\n",
        "    })\n",
        "\n",
        "df_rouge_results = pd.DataFrame(rouge_scores)\n",
        "\n",
        "# üîπ Calculate average ROUGE F1 scores\n",
        "rouge1_f1_avg = df_rouge_results[\"ROUGE-1 F1\"].mean()\n",
        "rouge2_f1_avg = df_rouge_results[\"ROUGE-2 F1\"].mean()\n",
        "rougeL_f1_avg = df_rouge_results[\"ROUGE-L F1\"].mean()\n",
        "\n",
        "# üìä Store evaluation results\n",
        "evaluation_results_gemini = pd.DataFrame({\n",
        "    \"Metric\": [\"MSE\", \"Pearson Correlation\", \"ROUGE-1 F1\", \"ROUGE-2 F1\", \"ROUGE-L F1\"],\n",
        "    \"Gemini vs VADER\": [mse_gemini, pearson_gemini, rouge1_f1_avg, rouge2_f1_avg, rougeL_f1_avg]\n",
        "})\n",
        "print(\"MSE: \" + str(mse_gemini))\n",
        "print(\"Pearson: \" + str(pearson_gemini))\n",
        "\n",
        "# ‚úÖ Save results to CSV\n",
        "evaluation_results_gemini.to_csv(\"evaluation_metrics_gemini_vs_vader.csv\", index=False)\n",
        "print(\"\\n‚úÖ Evaluation results saved to 'evaluation_metrics_gemini_vs_vader.csv'\")\n",
        "\n",
        "# üîπ Plot comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "metrics = evaluation_results_gemini[\"Metric\"]\n",
        "values = evaluation_results_gemini[\"Gemini vs VADER\"]\n",
        "\n",
        "plt.barh(metrics, values, alpha=0.7, color=\"purple\", label=\"Gemini vs VADER\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Metric\")\n",
        "plt.title(\"Gemini vs VADER: MSE, Pearson R, ROUGE F1 (Handling NaN)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# üìä Show plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# üîπ Load the reward scores CSV\n",
        "df_rewards = pd.read_csv(\"gemini_reward_scores_training.csv\")\n",
        "\n",
        "# üîπ Generate X-axis values (Number of closing prices processed)\n",
        "num_prices_processed = list(range(1, len(df_rewards) + 1))  # Sequential numbering\n",
        "\n",
        "# üîπ Extract Reward Scores\n",
        "reward_scores = df_rewards[\"Reward Score\"]\n",
        "\n",
        "# üîπ Plot the Reward Score Trend\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(num_prices_processed, reward_scores, marker=\"o\", linestyle=\"-\", color=\"blue\", label=\"Reward Score\")\n",
        "\n",
        "# üîπ Graph Labels & Customization\n",
        "plt.xlabel(\"Number of Closing Prices Processed\")\n",
        "plt.ylabel(\"Reward Score\")\n",
        "plt.title(\"Gemini Training Reward Score vs. Number of Articles Processed\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "\n",
        "# üìà Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ND1K8Q6HHZdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OD4jrVvC_vJh"
      },
      "outputs": [],
      "source": [
        "print(\"Claude Time taken = \" + str(totalClaudetime))\n",
        "print(\"GPT Time taken = \" + str(totalGPTtime))\n",
        "print(\"Gemini Time taken = \" + str(totalGeminitime))\n",
        "\n",
        "models = [\"Claude\", \"GPT-4\", \"Gemini\"]\n",
        "time_taken = [totalClaudetime, totalGPTtime, totalGeminitime]\n",
        "\n",
        "# Create bar plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(models, time_taken, color=['blue', 'red', 'green'])\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Model\")\n",
        "plt.ylabel(\"Time Taken (seconds)\")\n",
        "plt.title(\"Comparison of Time Taken for Claude, GPT-4, and Gemini\")\n",
        "\n",
        "# Show grid and plot\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}